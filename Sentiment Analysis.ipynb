{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Lots of libraries exist that will do sentiment analysis for you. Imagine that: just taking a sentence, throwing it into a library, and geting back a score! How convenient!\n",
    "\n",
    "It's also **totally irresponsible** unless you know how the sentiment analyzer was built. In this homework we're going to see how sentiment analysis is done with a few packages.\n",
    "\n",
    "## Installation\n",
    "\n",
    "If you haven't already, you'll want to `pip install` two packages: NLTK and Textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 5.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK: Natural Language Tooklit\n",
    "\n",
    "[Natural Language Toolkit](https://www.nltk.org/) is the basis for a lot of text analysis done in Python. It's old and terrible and slow, but it's just been used for so long and does so many things that it's generally the default when people get into text analysis. The new kid on the block is [spaCy](https://spacy.io/) (but it doesn't do sentiment analysis so we're leaving it out of this).\n",
    "\n",
    "When you first run NLTK, you need to download some datasets to make sure it will be able to do everything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/corina/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/corina/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/corina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do sentiment analysis with NLTK, it only takes a couple lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.153, 'neu': 0.688, 'pos': 0.159, 'compound': 0.0276}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "sia.polarity_scores(\"This restaurant was great, but I'm not sure if I'll go there again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking `SentimentIntensityAnalyzer` for the `polarity_score` gave us four values in a dictionary:\n",
    "\n",
    "- **negative:** the negative sentiment in a sentence\n",
    "- **neutral:** the neutral sentiment in a sentence\n",
    "- **positive:** the postivie sentiment in the sentence\n",
    "- **compound:** the aggregated sentiment. \n",
    "    \n",
    "Seems simple enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday?\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in real life, if you use an emoji you can be read as being more positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.4588}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday? :)\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday? ðŸ˜Š\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why didn't it understand the emoji the same way it understood the emoticon? Well, **it only knows the words that it's been trained on,** and if VADER's never seen ðŸ˜Š before it won't know what to think of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "\n",
    "TextBlob is built on top of NLTK, but is infinitely easier to use. It's still slow, but _it's so so so easy to use_. \n",
    "\n",
    "You can just feed TextBlob your sentence, then ask for a `.sentiment`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.275, subjectivity=0.8194444444444444)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"This restaurant was great, but I'm not sure if I'll go there again.\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How could it possibly be easier than that?!?!?** This time we get a `polarity` and a `subjectivity` instead of all of those different scores, but it's basically the same idea.\n",
    "\n",
    "If you like options: it turns out TextBlob actually has multiple sentiment analysis tools! How fun! We can plug in a different analyzer to get a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.5879425317005774, p_neg=0.41205746829942275)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"This restaurant was great, but I'm not sure if I'll go there again.\", analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a **very different result.** To understand why it's so different, we need to talk about where these sentiment numbers come from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But where do those numbers come from?\n",
    "\n",
    "The most important thing to understand is **sentiment is always just an opinion.** In this case it's an opinion, yes, but specifically **the opinion of a machine.**\n",
    "\n",
    "## VADER\n",
    "\n",
    "NLTK's Sentiment Intensity Analyzer works is using something called **VADER**, which is a list of words that have a sentiment associated with each of them.\n",
    "\n",
    "|Word|Sentiment rating|\n",
    "|---|---|\n",
    "|tragedy|-3.4|\n",
    "|rejoiced|2.0|\n",
    "|disaster|-3.1|\n",
    "|great|3.1|\n",
    "\n",
    "If you have more positives, the sentence is more positive. If you have more negatives, it's more negative. It can also take into account things like capitalization - you can read more about the classifier [here](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html), or the actual paper it came out of [here](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).\n",
    "\n",
    "**How do they know what's positive/negative?** They came up with a very big list of words, then asked people on the internet and paid them one cent for each word they scored.\n",
    "\n",
    "## TextBlob's `.sentiment`\n",
    "\n",
    "TextBlob's sentiment analysis is based on a separate library called [pattern](https://www.clips.uantwerpen.be/pattern).\n",
    "\n",
    "> The sentiment analysis lexicon bundled in Pattern focuses on adjectives. It contains adjectives that occur frequently in customer reviews, hand-tagged with values for polarity and subjectivity.\n",
    "\n",
    "Same kind of thing as NLTK's VADER, but it specifically looks at words from customer reviews.\n",
    "\n",
    "**How do they know what's positive/negative?** They look at (mostly) adjectives that occur in customer reviews and hand-tag them.\n",
    "\n",
    "## TextBlob's `.sentiment` + NaiveBayesAnalyzer\n",
    "\n",
    "TextBlob's other option uses a `NaiveBayesAnalyzer`, which is a machine learning technique. When you use this option with TextBlob, the sentiment is coming from \"an NLTK classifier trained on a movie reviews corpus.\"\n",
    "\n",
    "**How do they know what's positive/negative?** Looked at movie reviews and scores using machine learning, see what words are associated with a positive/negative rating.\n",
    "\n",
    "## What's this mean for me?\n",
    "\n",
    "When you're doing automatic sentiment analysis, you have two major questions: \n",
    "\n",
    "* Where does the list of known words come from\n",
    "* Where do the positive/negative scores come from\n",
    "\n",
    "Let's compare the tools we've used so far.\n",
    "\n",
    "|technique|word source|word selection|scores|\n",
    "|---|---|---|---|\n",
    "|NLTK (VADER)|everywhere|hand-picked|internet people, word-by-word|\n",
    "|TextBlob|product reviews|hand-picked, mostly adjectives|internet people, word-by-word|\n",
    "|TextBlob + NaiveBayesAnalyzer|movie reviews|all words|automatic based on score|\n",
    "\n",
    "A major thing that should jump out at you is **how different the sources are.**\n",
    "\n",
    "While VADER focuses on content found everywhere, TextBlob's two options are specific to certain domains. The [original paper for VADER](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf) passive-aggressively noted that VADER is effective at general use, but being trained on a specific domain can have benefits: \n",
    "\n",
    "> While some algorithms performed decently on test data from the specific domain for which it was expressly trained, they do not significantly outstrip the simple model we use.\n",
    "\n",
    "They're basically saying, \"if you train a model on words from a certain field, it will be good at that field.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1: Is it okay to use a sentiment analyzer built on product reviews to check the sentiment of tweets? How about to check the sentiment of wine reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#People on twitter can be as angry and aggressive as dissapointed consumers, but I guess it would be better to avoid\n",
    "#using it for tweets. Using it for wine reviews sounds proper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Is it okay to use a sentiment analyzer trained on everything to check the sentiment of tweets? How about to check the sentiment of wine reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems that it would work fine for both "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: If I'm trying to report on whether people generally like or dislike what is happening throughout the Democratic debates, could I use these sorts of tools on tweets? Let's hear arguments for both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't know if I would risk using these techniques for an actual story. The list of words may be thorough and big, \n",
    "#but the machine can't quite understand context. So there are chances that our analysis would be ambiguous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own sentiment analyzer\n",
    "\n",
    "We don't want to rely on other people, we want to do this ourselves! There are two major ways to do sentiment analysis:\n",
    "\n",
    "* Have a list of words that you humans assign positive or negative scores to\n",
    "* Look at something scored (movie reviews, product reviews) and figure out which words appear with which scores\n",
    "\n",
    "Depending on how you look at it, it's either a classification or a regression problem. We'll see the difference down below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on tweets\n",
    "\n",
    "Let's say we were going to analyze the sentiment of tweets. If we had a list of tweets that were scored positive vs. negative, we could see which words are usually associated with positive scores and which are usually associated with negative scores.\n",
    "\n",
    "Luckily, we have **Sentiment140** - http://help.sentiment140.com/for-students - a list of 1.6 million tweets along with a score as to whether they're negative (0) or positive (4). We'll use it to build our own machine learning algorithm to see separate positivity from negativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity          id                      datetime     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "          username                                            content  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['polarity', 'id', 'datetime', 'query', 'username', 'content']\n",
    "df = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", \n",
    "                 names=columns,\n",
    "                 encoding='latin-1')\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning our data\n",
    "\n",
    "The `polarity` field is whether something is positive or negative. How many do we have of each?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.polarity.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, `0` is negative and `4` is positive. Weird, right? Let's make it zero and one instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.polarity = df.polarity.replace(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm you have 800k of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    800000\n",
       "0    800000\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a **lot of tweets.**\n",
    "\n",
    "Let's be honest: it's going to take our algorithms a long long time to process that many. Instead of working with our entire dataframe, let's use a **sample of 20,000**. If things are still slow before we can decrease this number.\n",
    "\n",
    "* **Tip:** `df.sample(5)` will give you a sample of 5 elements of your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm you have 20,000 rows and 6 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 6)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize our tweets\n",
    "\n",
    "Create a `TfidfVectorizer` and use it to vectorize our tweets. Since we don't have all the time in the world, we should probably use `max_features` to only take a selection of terms - how about 2000 for now?\n",
    "\n",
    "* **Tip:** Your end result should be a `words_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zoo</th>\n",
       "      <th>Ã Â¹</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   09   10  100  1000   11   12   13   14  ...  yours  yourself  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
       "\n",
       "   youtube  yrs  yum  yummy  yup  zombie  zoo   Ã Â¹  \n",
       "0      0.0  0.0  0.0    0.0  0.0     0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0    0.0  0.0     0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0    0.0  0.0     0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0    0.0  0.0     0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0    0.0  0.0     0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 2000)\n",
    "vectors = vectorizer.fit_transform(df.content)\n",
    "words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your dataframe should look something like\n",
    "\n",
    "|00|000|10|...|your|...|yummy|yup|Â½t|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|0.0|0.0|0.0|...|0.0|...|0.0|0.0|0.0|\n",
    "|0.0|0.0|0.0|...|0.0|...|0.0|0.0|0.0|\n",
    "|0.0|0.0|0.0|...|0.235754|...|0.0|0.0|0.0|\n",
    "|0.0|0.0|0.0|...|0.0|...|0.0|0.0|0.0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our algorithm\n",
    "\n",
    "### Setting up our variables\n",
    "\n",
    "Create an `X` and a `y`, same as ever. In this case, what are our **features** and what are our **labels?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = words_df\n",
    "y = df.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that `X` has 20,000 rows and 2,000 columns, and that `y` has 20,000 rows of 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking an algorithm\n",
    "\n",
    "What kind of algorithm do we want? We've used quite a few, and I just pulled another one couple classifiers of thin air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When picking an algorithm, think about what the output should be: is it a category? A probability, an amount? In this case **it might be any of those!**\n",
    "\n",
    "For example:\n",
    "\n",
    "* **A category:** `0` or `1` for negative vs positive\n",
    "* **A probability:** The % chance that it's either negative or positive (between 0 and 1)\n",
    "* **An amount:** A score between 0 and 1 about how positive it is\n",
    "\n",
    "So hey, let's just make **one of each** of these. Name them `linreg`, `logreg`, `forest`, `svc`, and `bayes`.\n",
    "\n",
    "The two new ones - `LinearSVC` and `MultinomialNB` - work exactly the same as your other classifiers, you'll be doing the standard creation and fitting:\n",
    "\n",
    "```python\n",
    "svc = LinearSVC()\n",
    "svc.fit(X, y)\n",
    "```\n",
    "\n",
    "**Create and train classifiers in the cells below.** Add `%%time` to the top of each cell to see how long they take to train.\n",
    "\n",
    "* **Tip:** Remember you need to add `C=1e9` to logistic regression, and specify the solver!\n",
    "* **Tip:** If the logistic regression doesn't converge, it hasn't found an answer. You might need to increase `max_iter` (the default is 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 4.41 ms, total: 138 ms\n",
      "Wall time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a linear regression\n",
    "svc = LinearSVC()\n",
    "linreg = svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8019"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': True,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'loss': 'squared_hinge',\n",
       " 'max_iter': 1000,\n",
       " 'multi_class': 'ovr',\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = svc.predict(X_test)\n",
    "svc.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 s, sys: 413 ms, total: 23.4 s\n",
      "Wall time: 13.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a logistic regression - if it doesn't converge be sure to increase max_iter\n",
    "logreg = LogisticRegression(C=1e9, solver='lbfgs', max_iter=1000)\n",
    "logreg.fit(X_train.head(7000), y_train.head(7000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient (log odds ratio)</th>\n",
       "      <th>odds ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>aha</td>\n",
       "      <td>127.843972</td>\n",
       "      <td>3.326072e+55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>apple</td>\n",
       "      <td>106.848263</td>\n",
       "      <td>2.532860e+46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>dat</td>\n",
       "      <td>103.630553</td>\n",
       "      <td>1.014326e+45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>shop</td>\n",
       "      <td>92.178337</td>\n",
       "      <td>1.077812e+40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>saved</td>\n",
       "      <td>90.582213</td>\n",
       "      <td>2.184517e+39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>hug</td>\n",
       "      <td>87.185716</td>\n",
       "      <td>7.316030e+37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>xd</td>\n",
       "      <td>75.938195</td>\n",
       "      <td>9.539795e+32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>building</td>\n",
       "      <td>73.850140</td>\n",
       "      <td>1.182247e+32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>nah</td>\n",
       "      <td>72.032584</td>\n",
       "      <td>1.920232e+31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>flying</td>\n",
       "      <td>70.932307</td>\n",
       "      <td>6.390125e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>hah</td>\n",
       "      <td>67.773951</td>\n",
       "      <td>2.715518e+29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>weak</td>\n",
       "      <td>66.920933</td>\n",
       "      <td>1.157156e+29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>woo</td>\n",
       "      <td>65.770785</td>\n",
       "      <td>3.663440e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>bigger</td>\n",
       "      <td>65.409062</td>\n",
       "      <td>2.551494e+28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>art</td>\n",
       "      <td>63.230017</td>\n",
       "      <td>2.887005e+27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>tweeterfollow</td>\n",
       "      <td>62.142294</td>\n",
       "      <td>9.728716e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>nails</td>\n",
       "      <td>61.831229</td>\n",
       "      <td>7.127901e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>tweeteradder</td>\n",
       "      <td>61.670079</td>\n",
       "      <td>6.067012e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>texting</td>\n",
       "      <td>60.779156</td>\n",
       "      <td>2.489156e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>sexy</td>\n",
       "      <td>57.997746</td>\n",
       "      <td>1.542059e+25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>finishing</td>\n",
       "      <td>57.248781</td>\n",
       "      <td>7.291717e+24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>shining</td>\n",
       "      <td>56.163173</td>\n",
       "      <td>2.462386e+24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>yummy</td>\n",
       "      <td>54.818565</td>\n",
       "      <td>6.418011e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>alex</td>\n",
       "      <td>54.636512</td>\n",
       "      <td>5.349777e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>proud</td>\n",
       "      <td>54.381604</td>\n",
       "      <td>4.146012e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>fabulous</td>\n",
       "      <td>54.125907</td>\n",
       "      <td>3.210574e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>dressed</td>\n",
       "      <td>53.219818</td>\n",
       "      <td>1.297398e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09</td>\n",
       "      <td>52.933194</td>\n",
       "      <td>9.740786e+22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>normal</td>\n",
       "      <td>52.455285</td>\n",
       "      <td>6.040055e+22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>winter</td>\n",
       "      <td>49.942817</td>\n",
       "      <td>4.896548e+21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>tear</td>\n",
       "      <td>-30.835157</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>reality</td>\n",
       "      <td>-35.933766</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>hearing</td>\n",
       "      <td>-12.738833</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>available</td>\n",
       "      <td>-30.818812</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>hella</td>\n",
       "      <td>-34.145821</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>hurting</td>\n",
       "      <td>-48.421624</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>asap</td>\n",
       "      <td>-22.737858</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>hurts</td>\n",
       "      <td>-16.221005</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>ng</td>\n",
       "      <td>-18.576378</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>competition</td>\n",
       "      <td>-24.548275</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>completely</td>\n",
       "      <td>-20.628612</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>ate</td>\n",
       "      <td>-53.526433</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>aint</td>\n",
       "      <td>-21.078321</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>ran</td>\n",
       "      <td>-11.814207</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>confused</td>\n",
       "      <td>-12.028760</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>needs</td>\n",
       "      <td>-12.034365</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>tha</td>\n",
       "      <td>-28.134070</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>ain</td>\n",
       "      <td>-9.936221</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>vet</td>\n",
       "      <td>-45.927049</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>connection</td>\n",
       "      <td>-27.362199</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>neck</td>\n",
       "      <td>-10.092292</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>father</td>\n",
       "      <td>-9.961087</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>august</td>\n",
       "      <td>-35.456588</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>australia</td>\n",
       "      <td>-30.159829</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>vip</td>\n",
       "      <td>-50.036076</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>corner</td>\n",
       "      <td>-10.421089</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>cost</td>\n",
       "      <td>-23.783269</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>heaven</td>\n",
       "      <td>-11.677441</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>finals</td>\n",
       "      <td>-12.164551</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>fml</td>\n",
       "      <td>-31.035495</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature  coefficient (log odds ratio)    odds ratio\n",
       "65              aha                    127.843972  3.326072e+55\n",
       "123           apple                    106.848263  2.532860e+46\n",
       "448             dat                    103.630553  1.014326e+45\n",
       "1517           shop                     92.178337  1.077812e+40\n",
       "1466          saved                     90.582213  2.184517e+39\n",
       "875             hug                     87.185716  7.316030e+37\n",
       "1968             xd                     75.938195  9.539795e+32\n",
       "278        building                     73.850140  1.182247e+32\n",
       "1197            nah                     72.032584  1.920232e+31\n",
       "666          flying                     70.932307  6.390125e+30\n",
       "777             hah                     67.773951  2.715518e+29\n",
       "1880           weak                     66.920933  1.157156e+29\n",
       "1940            woo                     65.770785  3.663440e+28\n",
       "209          bigger                     65.409062  2.551494e+28\n",
       "134             art                     63.230017  2.887005e+27\n",
       "1793  tweeterfollow                     62.142294  9.728716e+26\n",
       "1199          nails                     61.831229  7.127901e+26\n",
       "1792   tweeteradder                     61.670079  6.067012e+26\n",
       "1694        texting                     60.779156  2.489156e+26\n",
       "1501           sexy                     57.997746  1.542059e+25\n",
       "653       finishing                     57.248781  7.291717e+24\n",
       "1510        shining                     56.163173  2.462386e+24\n",
       "1995          yummy                     54.818565  6.418011e+23\n",
       "80             alex                     54.636512  5.349777e+23\n",
       "1378          proud                     54.381604  4.146012e+23\n",
       "600        fabulous                     54.125907  3.210574e+23\n",
       "517         dressed                     53.219818  1.297398e+23\n",
       "2                09                     52.933194  9.740786e+22\n",
       "1232         normal                     52.455285  6.040055e+22\n",
       "1925         winter                     49.942817  4.896548e+21\n",
       "...             ...                           ...           ...\n",
       "1684           tear                    -30.835157  0.000000e+00\n",
       "1412        reality                    -35.933766  0.000000e+00\n",
       "818         hearing                    -12.738833  0.000000e+00\n",
       "150       available                    -30.818812  0.000000e+00\n",
       "827           hella                    -34.145821  0.000000e+00\n",
       "884         hurting                    -48.421624  0.000000e+00\n",
       "137            asap                    -22.737858  0.000000e+00\n",
       "885           hurts                    -16.221005  0.000000e+00\n",
       "1216             ng                    -18.576378  0.000000e+00\n",
       "384     competition                    -24.548275  0.000000e+00\n",
       "386      completely                    -20.628612  0.000000e+00\n",
       "145             ate                    -53.526433  0.000000e+00\n",
       "74             aint                    -21.078321  0.000000e+00\n",
       "1404            ran                    -11.814207  0.000000e+00\n",
       "390        confused                    -12.028760  0.000000e+00\n",
       "1209          needs                    -12.034365  0.000000e+00\n",
       "1696            tha                    -28.134070  0.000000e+00\n",
       "73              ain                     -9.936221  0.000000e+00\n",
       "1837            vet                    -45.927049  0.000000e+00\n",
       "393      connection                    -27.362199  0.000000e+00\n",
       "1206           neck                    -10.092292  0.000000e+00\n",
       "619          father                     -9.961087  0.000000e+00\n",
       "148          august                    -35.456588  0.000000e+00\n",
       "149       australia                    -30.159829  0.000000e+00\n",
       "1843            vip                    -50.036076  0.000000e+00\n",
       "402          corner                    -10.421089  0.000000e+00\n",
       "404            cost                    -23.783269  0.000000e+00\n",
       "821          heaven                    -11.677441  0.000000e+00\n",
       "645          finals                    -12.164551  0.000000e+00\n",
       "668             fml                    -31.035495  0.000000e+00\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = X.columns\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient (log odds ratio)': coefficients,\n",
    "    'odds ratio': np.exp(coefficients).round(4)\n",
    "}).sort_values(by='odds ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 101 ms, total: 4.14 s\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a random forest classifier\n",
    "random_for = RandomForestClassifier(n_estimators=10)\n",
    "random_for.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>feature importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>you</td>\n",
       "      <td>1.799613e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>not</td>\n",
       "      <td>1.450370e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>no</td>\n",
       "      <td>1.156147e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>my</td>\n",
       "      <td>1.088657e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>miss</td>\n",
       "      <td>1.039346e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>good</td>\n",
       "      <td>1.027232e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>sad</td>\n",
       "      <td>1.007804e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>to</td>\n",
       "      <td>1.000452e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>the</td>\n",
       "      <td>9.819631e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>thanks</td>\n",
       "      <td>9.501061e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>for</td>\n",
       "      <td>8.169053e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>but</td>\n",
       "      <td>8.025597e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>is</td>\n",
       "      <td>7.971536e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>love</td>\n",
       "      <td>7.582356e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>me</td>\n",
       "      <td>7.454958e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>it</td>\n",
       "      <td>7.223898e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>in</td>\n",
       "      <td>6.923017e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>and</td>\n",
       "      <td>6.920933e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>work</td>\n",
       "      <td>6.874932e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>that</td>\n",
       "      <td>6.277095e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>have</td>\n",
       "      <td>6.262393e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>wish</td>\n",
       "      <td>5.654636e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>with</td>\n",
       "      <td>5.521894e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>sorry</td>\n",
       "      <td>5.510591e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>bad</td>\n",
       "      <td>5.423034e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>of</td>\n",
       "      <td>5.323170e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>so</td>\n",
       "      <td>5.250386e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>your</td>\n",
       "      <td>5.154649e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>on</td>\n",
       "      <td>4.871335e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>want</td>\n",
       "      <td>4.785086e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>father</td>\n",
       "      <td>1.887150e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>upgrade</td>\n",
       "      <td>1.737546e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>throw</td>\n",
       "      <td>1.225943e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>sam</td>\n",
       "      <td>1.217190e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>pop</td>\n",
       "      <td>6.399450e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>crappy</td>\n",
       "      <td>5.271134e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>daily</td>\n",
       "      <td>5.213839e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>id</td>\n",
       "      <td>4.904195e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>aha</td>\n",
       "      <td>4.430479e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>alas</td>\n",
       "      <td>3.327068e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>sense</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>zombie</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>piece</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>peter</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>joke</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>smart</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>smoking</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>hw</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>soup</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>germany</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>report</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>bigger</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>form</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>talked</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>episodes</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>competition</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>butt</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>bucks</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>praying</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>media</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  feature importance\n",
       "1987          you        1.799613e-02\n",
       "1235          not        1.450370e-02\n",
       "1224           no        1.156147e-02\n",
       "1192           my        1.088657e-02\n",
       "1144         miss        1.039346e-02\n",
       "741          good        1.027232e-02\n",
       "1454          sad        1.007804e-02\n",
       "1743           to        1.000452e-02\n",
       "1703          the        9.819631e-03\n",
       "1699       thanks        9.501061e-03\n",
       "678           for        8.169053e-03\n",
       "288           but        8.025597e-03\n",
       "919            is        7.971536e-03\n",
       "1064         love        7.582356e-03\n",
       "1107           me        7.454958e-03\n",
       "924            it        7.223898e-03\n",
       "904            in        6.923017e-03\n",
       "101           and        6.920933e-03\n",
       "1946         work        6.874932e-03\n",
       "1701         that        6.277095e-03\n",
       "805          have        6.262393e-03\n",
       "1926         wish        5.654636e-03\n",
       "1930         with        5.521894e-03\n",
       "1590        sorry        5.510591e-03\n",
       "170           bad        5.423034e-03\n",
       "1245           of        5.323170e-03\n",
       "1570           so        5.250386e-03\n",
       "1989         your        5.154649e-03\n",
       "1259           on        4.871335e-03\n",
       "1858         want        4.785086e-03\n",
       "...           ...                 ...\n",
       "619        father        1.887150e-07\n",
       "1822      upgrade        1.737546e-07\n",
       "1726        throw        1.225943e-07\n",
       "1459          sam        1.217190e-07\n",
       "1350          pop        6.399450e-08\n",
       "420        crappy        5.271134e-08\n",
       "439         daily        5.213839e-08\n",
       "890            id        4.904195e-08\n",
       "65            aha        4.430479e-08\n",
       "78           alas        3.327068e-08\n",
       "1492        sense        0.000000e+00\n",
       "1997       zombie        0.000000e+00\n",
       "1325        piece        0.000000e+00\n",
       "1313        peter        0.000000e+00\n",
       "942          joke        0.000000e+00\n",
       "1561        smart        0.000000e+00\n",
       "1567      smoking        0.000000e+00\n",
       "887            hw        0.000000e+00\n",
       "1594         soup        0.000000e+00\n",
       "718       germany        0.000000e+00\n",
       "1426       report        0.000000e+00\n",
       "209        bigger        0.000000e+00\n",
       "682          form        0.000000e+00\n",
       "1675       talked        0.000000e+00\n",
       "565      episodes        0.000000e+00\n",
       "384   competition        0.000000e+00\n",
       "289          butt        0.000000e+00\n",
       "273         bucks        0.000000e+00\n",
       "1361      praying        0.000000e+00\n",
       "1111        media        0.000000e+00\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X.columns\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'feature importance': importances,\n",
    "}).sort_values(by='feature importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 0 ns, total: 2 Âµs\n",
      "Wall time: 5.01 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a linear support vector classifier (LinearSVC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a multinomial naive bayes classifier (MultinomialNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How long did each take to train?** How much faster were some compared to others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use our models on some new data\n",
    "\n",
    "Now that we've trained our models, **they can try to predict whether a model is positive or negative**.\n",
    "\n",
    "**Add five more sentences to the list below.** They should be a mix of positive and negative. They can be boring, they can be exciting, they can be short, they can be long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm not sure how I feel about toast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did you see the baseball game yesterday?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The package was delivered late and the contents were broken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trashy television shows are some of my favorites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I find chirping birds irritating, but I know I'm not the only one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    content\n",
       "0                                       I'm not sure how I feel about toast\n",
       "1                                  Did you see the baseball game yesterday?\n",
       "2               The package was delivered late and the contents were broken\n",
       "3                          Trashy television shows are some of my favorites\n",
       "4  I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\n",
       "5         I find chirping birds irritating, but I know I'm not the only one"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create some test data\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "unknown = pd.DataFrame([\n",
    "       \"I'm not sure how I feel about toast\",\n",
    "       \"Did you see the baseball game yesterday?\",\n",
    "       \"The package was delivered late and the contents were broken\",\n",
    "       \"Trashy television shows are some of my favorites\",\n",
    "       \"I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\",\n",
    "       \"I find chirping birds irritating, but I know I'm not the only one\"\n",
    "], columns=['content'])\n",
    "unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to **vectorizer** our sentences into numbers, so the algorithm can understand them.\n",
    "\n",
    "Our algorithm only knows **certain words.** Run `vectorizer.get_feature_names()` to show you the list of the words it knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '140',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '1st',\n",
       " '20',\n",
       " '2009',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '2am',\n",
       " '2day',\n",
       " '2morrow',\n",
       " '2nd',\n",
       " '2nite',\n",
       " '30',\n",
       " '33',\n",
       " '3am',\n",
       " '3gs',\n",
       " '3rd',\n",
       " '40',\n",
       " '45',\n",
       " '4th',\n",
       " '50',\n",
       " '6am',\n",
       " 'able',\n",
       " 'about',\n",
       " 'absolutely',\n",
       " 'abt',\n",
       " 'accident',\n",
       " 'according',\n",
       " 'account',\n",
       " 'ache',\n",
       " 'across',\n",
       " 'acting',\n",
       " 'actually',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'adorable',\n",
       " 'advice',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'ages',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahaha',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhh',\n",
       " 'aim',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'al',\n",
       " 'alas',\n",
       " 'album',\n",
       " 'alex',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allowed',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amp',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'and',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angry',\n",
       " 'anniversary',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'apple',\n",
       " 'appreciate',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'area',\n",
       " 'aren',\n",
       " 'argh',\n",
       " 'arm',\n",
       " 'arms',\n",
       " 'around',\n",
       " 'arrived',\n",
       " 'art',\n",
       " 'article',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'aswell',\n",
       " 'at',\n",
       " 'ate',\n",
       " 'atl',\n",
       " 'attention',\n",
       " 'august',\n",
       " 'australia',\n",
       " 'available',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'awwww',\n",
       " 'b4',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'background',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'ball',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bank',\n",
       " 'bar',\n",
       " 'barely',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'battery',\n",
       " 'bb',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bday',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'bedtime',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'before',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'belly',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'better',\n",
       " 'between',\n",
       " 'bf',\n",
       " 'bff',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'bing',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blanket',\n",
       " 'blast',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blip',\n",
       " 'block',\n",
       " 'blog',\n",
       " 'blogspot',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'body',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boom',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'both',\n",
       " 'bothered',\n",
       " 'bottle',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'brain',\n",
       " 'brazil',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'bright',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'bro',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'btw',\n",
       " 'bubble',\n",
       " 'bucks',\n",
       " 'buddy',\n",
       " 'bug',\n",
       " 'bugs',\n",
       " 'build',\n",
       " 'building',\n",
       " 'bummed',\n",
       " 'bummer',\n",
       " 'bunch',\n",
       " 'burn',\n",
       " 'burning',\n",
       " 'burnt',\n",
       " 'bus',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'ca',\n",
       " 'cake',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'camping',\n",
       " 'can',\n",
       " 'canada',\n",
       " 'cancel',\n",
       " 'candy',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catching',\n",
       " 'cats',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'cd',\n",
       " 'cell',\n",
       " 'ch',\n",
       " 'chair',\n",
       " 'challenge',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'channel',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheer',\n",
       " 'cheers',\n",
       " 'cheese',\n",
       " 'chicago',\n",
       " 'chicken',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chill',\n",
       " 'chillin',\n",
       " 'chinese',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chris',\n",
       " 'church',\n",
       " 'city',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'click',\n",
       " 'client',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closer',\n",
       " 'clothes',\n",
       " 'clouds',\n",
       " 'cloudy',\n",
       " 'club',\n",
       " 'co',\n",
       " 'coast',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'college',\n",
       " 'color',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'comp',\n",
       " 'company',\n",
       " 'competition',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'computer',\n",
       " 'conan',\n",
       " 'concert',\n",
       " 'confused',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'connection',\n",
       " 'control',\n",
       " 'convo',\n",
       " 'cook',\n",
       " 'cookie',\n",
       " 'cookies',\n",
       " 'cooking',\n",
       " 'cool',\n",
       " 'copy',\n",
       " 'corner',\n",
       " 'cos',\n",
       " 'cost',\n",
       " 'couch',\n",
       " 'cough',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'couldnt',\n",
       " 'count',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cousin',\n",
       " 'cousins',\n",
       " 'cover',\n",
       " 'covered',\n",
       " 'coz',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'crash',\n",
       " 'craving',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'credit',\n",
       " 'cried',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'cuddle',\n",
       " 'cup',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'daily',\n",
       " 'dammit',\n",
       " 'damn',\n",
       " 'dance',\n",
       " 'dancing',\n",
       " 'dang',\n",
       " 'danny',\n",
       " 'dark',\n",
       " 'darn',\n",
       " 'dat',\n",
       " 'data',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'david',\n",
       " 'davidarchie',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dc',\n",
       " 'ddlovato',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'decent',\n",
       " 'decided',\n",
       " 'def',\n",
       " 'definitely',\n",
       " 'degrees',\n",
       " 'delicious',\n",
       " 'demi',\n",
       " 'dentist',\n",
       " 'depressed',\n",
       " 'depressing',\n",
       " 'design',\n",
       " 'desk',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'diet',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'direct',\n",
       " 'disappointed',\n",
       " 'disney',\n",
       " 'diversity',\n",
       " 'dj',\n",
       " 'dm',\n",
       " 'dnt',\n",
       " 'do',\n",
       " 'doc',\n",
       " 'doctor',\n",
       " 'doctors',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'doin',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'done',\n",
       " 'donniewahlberg',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'down',\n",
       " 'download',\n",
       " 'downtown',\n",
       " 'dr',\n",
       " 'drank',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'dress',\n",
       " 'dressed',\n",
       " 'drink',\n",
       " 'drinking',\n",
       " 'drinks',\n",
       " 'drive',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'dropped',\n",
       " 'drunk',\n",
       " 'dry',\n",
       " 'dude',\n",
       " 'due',\n",
       " 'dumb',\n",
       " 'dunno',\n",
       " 'during',\n",
       " 'duty',\n",
       " 'dvd',\n",
       " 'dying',\n",
       " 'each',\n",
       " 'ear',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eaten',\n",
       " 'eating',\n",
       " 'effort',\n",
       " 'egg',\n",
       " 'eh',\n",
       " 'either',\n",
       " 'else',\n",
       " 'em',\n",
       " 'email',\n",
       " 'emails',\n",
       " 'empty',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'energy',\n",
       " 'england',\n",
       " 'english',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enough',\n",
       " 'epic',\n",
       " 'episode',\n",
       " 'episodes',\n",
       " 'er',\n",
       " 'error',\n",
       " 'especially',\n",
       " 'essay',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'evil',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'exam',\n",
       " 'exams',\n",
       " 'excellent',\n",
       " 'except',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'exhausted',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'eyes',\n",
       " 'fab',\n",
       " 'fabulous',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fail',\n",
       " 'failed',\n",
       " 'fair',\n",
       " 'fall',\n",
       " 'falling',\n",
       " 'fam',\n",
       " 'family',\n",
       " 'famous',\n",
       " 'fan',\n",
       " 'fancy',\n",
       " 'fans',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'fat',\n",
       " 'father',\n",
       " 'fault',\n",
       " 'fav',\n",
       " 'fave',\n",
       " 'favorite',\n",
       " 'favourite',\n",
       " 'fb',\n",
       " 'feed',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feels',\n",
       " 'feet',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'festival',\n",
       " 'fever',\n",
       " 'few',\n",
       " 'ff',\n",
       " 'field',\n",
       " 'fight',\n",
       " 'fighting',\n",
       " 'figure',\n",
       " 'figured',\n",
       " 'film',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'finals',\n",
       " 'find',\n",
       " 'finding',\n",
       " 'fine',\n",
       " 'finger',\n",
       " 'fingers',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'finishing',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'fit',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'fixed',\n",
       " 'flat',\n",
       " 'flight',\n",
       " 'floor',\n",
       " 'flu',\n",
       " 'fly',\n",
       " 'flying',\n",
       " 'fm',\n",
       " 'fml',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'followed',\n",
       " 'followers',\n",
       " 'followfriday',\n",
       " 'following',\n",
       " 'food',\n",
       " 'foot',\n",
       " 'football',\n",
       " 'for',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'form',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'france',\n",
       " 'freak',\n",
       " 'freakin',\n",
       " 'freaking',\n",
       " 'free',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'fried',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'from',\n",
       " 'front',\n",
       " 'frustrated',\n",
       " 'ftw',\n",
       " 'fuck',\n",
       " 'fucked',\n",
       " 'fuckin',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'garden',\n",
       " 'gave',\n",
       " 'gay',\n",
       " 'gd',\n",
       " 'general',\n",
       " 'genius',\n",
       " 'george',\n",
       " 'germany',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'gettin',\n",
       " 'getting',\n",
       " 'gf',\n",
       " 'gift',\n",
       " 'gig',\n",
       " 'girl',\n",
       " 'girlfriend',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'glasses',\n",
       " 'go',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'goodness',\n",
       " 'goodnight',\n",
       " 'google',\n",
       " 'gorgeous',\n",
       " 'gosh',\n",
       " 'got',\n",
       " 'gotta',\n",
       " 'gotten',\n",
       " 'grab',\n",
       " 'grace',\n",
       " 'grad',\n",
       " 'grade',\n",
       " 'graduation',\n",
       " 'grand',\n",
       " 'grandma',\n",
       " 'grass',\n",
       " 'great',\n",
       " 'green',\n",
       " 'grey',\n",
       " 'grocery',\n",
       " 'gross',\n",
       " 'group',\n",
       " 'grrr',\n",
       " 'gt',\n",
       " 'guess',\n",
       " 'guilty',\n",
       " 'guitar',\n",
       " 'gunna',\n",
       " 'gutted',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gym',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'hah',\n",
       " 'haha',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hahahaha',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'hands',\n",
       " 'hang',\n",
       " 'hanging',\n",
       " 'hangover',\n",
       " 'hannah',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'harry',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'hat',\n",
       " 'hate',\n",
       " 'hated',\n",
       " 'hates',\n",
       " 'hav',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'havent',\n",
       " 'having',\n",
       " 'hd',\n",
       " 'he',\n",
       " 'head',\n",
       " 'headache',\n",
       " 'headed',\n",
       " 'heading',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'hearing',\n",
       " 'heart',\n",
       " 'heat',\n",
       " 'heaven',\n",
       " 'heck',\n",
       " 'heh',\n",
       " 'hehe',\n",
       " 'hehehe',\n",
       " 'hell',\n",
       " 'hella',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helping',\n",
       " 'helps',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hes',\n",
       " 'hey',\n",
       " 'heyy',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'hilarious',\n",
       " 'hill',\n",
       " 'hills',\n",
       " 'him',\n",
       " 'his',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hmm',\n",
       " 'hmmm',\n",
       " 'ho',\n",
       " 'hold',\n",
       " 'holding',\n",
       " 'hole',\n",
       " 'holiday',\n",
       " 'holidays',\n",
       " 'holy',\n",
       " 'home',\n",
       " 'homework',\n",
       " 'honey',\n",
       " 'hope',\n",
       " 'hopefully',\n",
       " 'hoping',\n",
       " 'horrible',\n",
       " 'hospital',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hows',\n",
       " 'hrs',\n",
       " 'http',\n",
       " 'hubby',\n",
       " 'hug',\n",
       " 'huge',\n",
       " 'hugs',\n",
       " 'huh',\n",
       " 'human',\n",
       " 'hun',\n",
       " 'hungry',\n",
       " 'hurry',\n",
       " 'hurt',\n",
       " 'hurting',\n",
       " 'hurts',\n",
       " 'husband',\n",
       " 'hw',\n",
       " 'ice',\n",
       " 'icecream',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'idiot',\n",
       " 'idk',\n",
       " 'if',\n",
       " 'ill',\n",
       " 'ily',\n",
       " 'im',\n",
       " 'ima',\n",
       " 'imagine',\n",
       " 'imma',\n",
       " 'important',\n",
       " 'impressed',\n",
       " 'in',\n",
       " 'including',\n",
       " 'indeed',\n",
       " 'info',\n",
       " 'inside',\n",
       " 'install',\n",
       " 'instead',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'into',\n",
       " 'invite',\n",
       " 'iphone',\n",
       " 'ipod',\n",
       " 'is',\n",
       " 'island',\n",
       " 'isn',\n",
       " 'isnt',\n",
       " 'issues',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itunes',\n",
       " 'ive',\n",
       " 'jack',\n",
       " 'jam',\n",
       " 'jason',\n",
       " 'jay',\n",
       " 'jb',\n",
       " 'jealous',\n",
       " 'jess',\n",
       " 'jesus',\n",
       " 'job',\n",
       " 'jobs',\n",
       " 'joe',\n",
       " 'john',\n",
       " 'join',\n",
       " 'joined',\n",
       " 'joke',\n",
       " 'jon',\n",
       " 'jonas',\n",
       " 'jonasbrothers',\n",
       " 'jonathanrknight',\n",
       " 'josh',\n",
       " 'joy',\n",
       " 'juice',\n",
       " 'july',\n",
       " 'june',\n",
       " 'jus',\n",
       " 'just',\n",
       " 'kate',\n",
       " 'keep',\n",
       " 'keeping',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'key',\n",
       " 'kick',\n",
       " 'kicked',\n",
       " 'kid',\n",
       " 'kidding',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'king',\n",
       " 'kiss',\n",
       " 'kitchen',\n",
       " 'kitty',\n",
       " 'knee',\n",
       " 'knew',\n",
       " 'kno',\n",
       " 'know',\n",
       " 'knowing',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'la',\n",
       " 'ladies',\n",
       " 'lady',\n",
       " 'lake',\n",
       " 'lakers',\n",
       " 'lame',\n",
       " 'land',\n",
       " 'laptop',\n",
       " 'last',\n",
       " 'late',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'laugh',\n",
       " 'laughing',\n",
       " 'launch',\n",
       " 'laundry',\n",
       " 'law',\n",
       " 'lay',\n",
       " ...]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually when we use the vectorizer, we write code like this:\n",
    "    \n",
    "```python\n",
    "vectors = vectorizer.fit_transform(....)\n",
    "```\n",
    "\n",
    "Which both learns all the words **and** counts them. In this case **we already have the list of words we know, we only want to count them.** So instead of `.fit_transform`, we just use `.transform`:\n",
    "\n",
    "```python\n",
    "unknown_vectors = vectorizer.transform(unknown.content)\n",
    "unknown_words_df = ......\n",
    "```\n",
    "\n",
    "Finish making your `unknown_words_df` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_df = vectorizer.transform(unknown.content)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm `unknown_words_df` is 11 rows and 2,000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2000)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with our models\n",
    "\n",
    "To make a prediction for each of our sentences, you can use `.predict` with each of our models. For example, it would look like this for linear regression:\n",
    "\n",
    "```python\n",
    "unknown['pred_linreg'] = linreg.predict(unknown_words_df)\n",
    "```\n",
    "\n",
    "To add the prediction for logistic regression, you'd run similar `.predict` code, which will give you a `0` (negative) or a `1` (positive). A difference between the two is that for logistic regression, you can **also ask for the probability that the sentence is in the `1` category** instead of just simply the category. To do that, you use this code:\n",
    "\n",
    "```python\n",
    "unknown['pred_logreg_prob'] = linreg.predict_proba(unknown_words_df)[:,1]\n",
    "```\n",
    "\n",
    "**Add new columns for each of the models you trained.** If the model has a `.predict_proba`, add that as a column as well. \n",
    "\n",
    "* **Tip:** Tab is helpful for knowing whether `.predict_proba` is an option.\n",
    "* **Tip:** Don't forget the `[:,1]` after `.predict_proba`, it means \"give me the probability for category `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown['pred_linreg'] = linreg.predict(unknown_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown['pred_logreg'] = logreg.predict(unknown_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown['pred_random'] = random_for.predict(unknown_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>pred_linreg</th>\n",
       "      <th>pred_logreg</th>\n",
       "      <th>pred_random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm not sure how I feel about toast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did you see the baseball game yesterday?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The package was delivered late and the contents were broken</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trashy television shows are some of my favorites</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I find chirping birds irritating, but I know I'm not the only one</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    content  \\\n",
       "0                                       I'm not sure how I feel about toast   \n",
       "1                                  Did you see the baseball game yesterday?   \n",
       "2               The package was delivered late and the contents were broken   \n",
       "3                          Trashy television shows are some of my favorites   \n",
       "4  I'm seeing a Kubrick film tomorrow, I hear not so great things about it.   \n",
       "5         I find chirping birds irritating, but I know I'm not the only one   \n",
       "\n",
       "   pred_linreg  pred_logreg  pred_random  \n",
       "0            0            0            0  \n",
       "1            1            1            1  \n",
       "2            0            0            0  \n",
       "3            0            1            0  \n",
       "4            1            1            0  \n",
       "5            0            0            0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look something like the below. Check your column names to confirm they match up.\n",
    "\n",
    "|content|pred_linreg|pred_logreg|pred_logreg_proba|pred_forest|pred_forest_proba|pred_svc|pred_bayes|pred_bayes_proba|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|I'm not sure how I feel about toast|0.342560|0|0.271403|0|0.5|0|0|0.425271|\n",
    "|...|...|...|...|...|...|...|...|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What do the numbers mean? What's the difference between a 0 and a 1? A 0.5? Negative numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Were there any sentences where the classifiers seemed to disagree about? How do you feel about the amount they disagree? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: What's the difference between using a 0/1 to talk about sentiment compared to 0-1? When might you use one compared to another?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: What's the difference between the linear regression model and the other models we're using? Why might it fit or not fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Between 0-1, what range do you think counts as \"negative,\" \"positive\" and \"neutral\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Does the variation in scores reflect the variation you would see among people? Or is it better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe we should have tested this?\n",
    "\n",
    "We can actually see **which model performs the best**. Let's remind ourselves what we have by looking at:\n",
    "\n",
    "* `X`\n",
    "* `y`\n",
    "* `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original dataframe is a list of many, many tweets. We turned this into `X` - vectorized words - and `y` - whether the tweet is negative or positive.\n",
    "\n",
    "Before we used `.fit(X, y)` to train on all of our data. Instead, **we can test our models** by doing a test/train split and see if the predictions match the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test and training data \n",
    "\n",
    "Split your `X` and `y` into train and test datasets. I always have to look up how to do it, so here's the code for you:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `X_train` and `y_train` to train all your models, except the linear regression one. You should be training:\n",
    "\n",
    "* `logreg`\n",
    "* `forest`\n",
    "* `svc`\n",
    "* `bayes`\n",
    "\n",
    "Again, do them each in **separate cells** and use `%%time` to see how long each one takes to learn what's a positive vs negative tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrices\n",
    "\n",
    "To see how well they did, we'll use a confusion matrix for each one. For example, here is what you'll use for logistic regression:\n",
    "\n",
    "```python\n",
    "y_true = y_test\n",
    "y_pred = logreg.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage-based confusion matrices\n",
    "\n",
    "Those are kind of irritating in that they're just numbers. It might work better if you do something like this instead to get percentages:\n",
    "\n",
    "```python\n",
    "y_true = y_test\n",
    "y_pred = logreg.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names) / matrix.sum(axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Which models performed the best? Were there big differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Do you think it's more important to be sensitive to negativity or positivity? Do we want more positive things incorrectly marked as negative, or more negative things marked as positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: They all had very different training times. Which ones offer the best combination of performance and not making you wait around for an hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13: If you have a decent algorithm that trains more quickly, that could that mean about feature selection or the size of your training set? Why did we use `max_features=` and `df.sample`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14: How do you feel about sentiment analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15: How do you feel about [this piece from the UpShot](https://www.nytimes.com/interactive/2017/02/28/upshot/trump-sounds-different-tone-in-first-address-to-congress.html) that uses [the Emotional Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16: What would you feel comfortable using our sentiment classifier for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
